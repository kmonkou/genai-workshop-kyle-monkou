{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install Packages"
      ],
      "metadata": {
        "id": "IFYNhObPmPkX"
      },
      "id": "IFYNhObPmPkX"
    },
    {
      "cell_type": "code",
      "id": "oP4q7tFzRordV60wJzJF1NA1",
      "metadata": {
        "tags": [],
        "id": "oP4q7tFzRordV60wJzJF1NA1"
      },
      "source": [
        "# ‚úÖ Install / upgrade required libraries (Vertex AI & BigQuery client)\n",
        "%pip install --upgrade google-cloud-bigquery google-cloud-bigquery-storage google-cloud-aiplatform --quiet\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup Variables"
      ],
      "metadata": {
        "id": "ARJ0SuromxXb"
      },
      "id": "ARJ0SuromxXb"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import aiplatform\n",
        "from google.colab import auth\n",
        "\n",
        "# üîë Authenticate to GCP (in Vertex AI Colab Enterprise this might be implicit, but this is safe)\n",
        "auth.authenticate_user()\n",
        "\n",
        "# üîß Set your GCP project, region, and BigQuery dataset/table names\n",
        "PROJECT_ID = \"qwiklabs-gcp-01-562fabefbdb6\"\n",
        "LOCATION = \"us-central1\"   # or the region where you're running Gemini\n",
        "BQ_LOCATION = \"US\"         # BigQuery dataset location\n",
        "\n",
        "DATASET_ID = \"aurora_bay\"          # choose a dataset name\n",
        "TABLE_ID = \"faqs\"                  # original FAQs table\n",
        "EMBEDDING_TABLE_ID = \"faqs_embed\"  # table that will include embeddings\n",
        "\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "\n",
        "# Initialize Vertex AI\n",
        "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "# Create BigQuery client\n",
        "bq_client = bigquery.Client(project=PROJECT_ID, location=BQ_LOCATION)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdzU84uxm0dW",
        "outputId": "8ce7b923-a721-4e31-c6b6-c6e7fa81eb26"
      },
      "id": "NdzU84uxm0dW",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/google/cloud/aiplatform/models.py:52: FutureWarning: Support for google-cloud-storage < 3.0.0 will be removed in a future version of google-cloud-aiplatform. Please upgrade to google-cloud-storage >= 3.0.0.\n",
            "  from google.cloud.aiplatform.utils import gcs_utils\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: google.colab.auth.authenticate_user() is not supported in Colab Enterprise.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the CSV from GCS into BigQuery"
      ],
      "metadata": {
        "id": "08GmddKfncnt"
      },
      "id": "08GmddKfncnt"
    },
    {
      "cell_type": "code",
      "source": [
        "# üì• GCS path to the lab file\n",
        "gcs_uri = \"gs://labs.roitraining.com/aurora-bay-faqs/aurora-bay-faqs.csv\"\n",
        "\n",
        "# Define dataset (create if it doesn't exist)\n",
        "dataset_ref = bigquery.Dataset(f\"{PROJECT_ID}.{DATASET_ID}\")\n",
        "dataset_ref.location = BQ_LOCATION\n",
        "\n",
        "try:\n",
        "    dataset_ref = bq_client.create_dataset(dataset_ref)  # will fail if exists\n",
        "    print(f\"Created dataset {dataset_ref.full_dataset_id}\")\n",
        "except Exception as e:\n",
        "    print(f\"Dataset may already exist: {e}\")\n",
        "\n",
        "# Define load job: CSV ‚Üí BigQuery\n",
        "table_ref = f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n",
        "\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    source_format=bigquery.SourceFormat.CSV,\n",
        "    skip_leading_rows=1,\n",
        "    autodetect=True,\n",
        ")\n",
        "\n",
        "load_job = bq_client.load_table_from_uri(\n",
        "    gcs_uri,\n",
        "    table_ref,\n",
        "    job_config=job_config,\n",
        ")\n",
        "\n",
        "print(\"‚è≥ Loading CSV into BigQuery...\")\n",
        "load_job.result()\n",
        "print(\"‚úÖ Load complete\")\n",
        "\n",
        "table = bq_client.get_table(table_ref)\n",
        "print(\"Loaded rows:\", table.num_rows)\n",
        "print(\"Schema:\", table.schema)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_ftArYwndKl",
        "outputId": "80541179-5b08-4ce5-f242-e2ba4401a1ed"
      },
      "id": "j_ftArYwndKl",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset qwiklabs-gcp-01-562fabefbdb6:aurora_bay\n",
            "‚è≥ Loading CSV into BigQuery...\n",
            "‚úÖ Load complete\n",
            "Loaded rows: 50\n",
            "Schema: [SchemaField('string_field_0', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('string_field_1', 'STRING', 'NULLABLE', None, None, (), None)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create an embeddings table in BigQuery"
      ],
      "metadata": {
        "id": "zetQVWdZnkAc"
      },
      "id": "zetQVWdZnkAc"
    },
    {
      "cell_type": "code",
      "source": [
        "schema = [\n",
        "    bigquery.SchemaField(\"faq_id\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"question\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"answer\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    bigquery.SchemaField(\"combined_text\", \"STRING\", mode=\"REQUIRED\"),\n",
        "    # Store embedding as an ARRAY<FLOAT64>\n",
        "    bigquery.SchemaField(\"embedding\", \"FLOAT64\", mode=\"REPEATED\"),\n",
        "]\n",
        "\n",
        "embedding_table_ref = bigquery.Table(\n",
        "    f\"{PROJECT_ID}.{DATASET_ID}.{EMBEDDING_TABLE_ID}\",\n",
        "    schema=schema,\n",
        ")\n",
        "\n",
        "try:\n",
        "    embedding_table_ref = bq_client.create_table(embedding_table_ref)\n",
        "    print(f\"‚úÖ Created embedding table {embedding_table_ref.full_table_id}\")\n",
        "except Exception as e:\n",
        "    print(f\"Embedding table may already exist: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOwMKxdFnfOo",
        "outputId": "88226848-607e-42df-c828-96971ed9b9cc"
      },
      "id": "qOwMKxdFnfOo",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Created embedding table qwiklabs-gcp-01-562fabefbdb6:aurora_bay.faqs_embed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate embeddings with Gemini and write them back to BigQuery\n"
      ],
      "metadata": {
        "id": "jtDnwPDwnssW"
      },
      "id": "jtDnwPDwnssW"
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.language_models import TextEmbeddingModel\n",
        "from google.cloud import bigquery # Assuming bq_client is an instance of bigquery.Client\n",
        "\n",
        "# üî¢ Choose an embedding model\n",
        "EMBEDDING_MODEL_NAME = \"text-embedding-004\"\n",
        "\n",
        "embedding_model = TextEmbeddingModel.from_pretrained(EMBEDDING_MODEL_NAME)\n",
        "\n",
        "# Assuming PROJECT_ID, DATASET_ID, TABLE_ID, EMBEDDING_TABLE_ID are defined\n",
        "# and bq_client is initialized correctly:\n",
        "# bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "# Optional: You can get the table schema if needed, but the error suggests direct column name issue\n",
        "# source_table = bq_client.get_table(table_ref)\n",
        "\n",
        "# Fetch all rows for simplicity (for larger datasets, do this in batches / with query)\n",
        "\n",
        "# This first query is fine if it correctly identifies your Q&A columns\n",
        "# You might want to remove this if the second query is meant to be the definitive one\n",
        "QUESTION_COL = \"string_field_0\"\n",
        "ANSWER_COL = \"string_field_1\"\n",
        "\n",
        "# The first query was for demonstrating column names, but the second one is where the issue is.\n",
        "# Let's focus on fixing the second query used for generating embeddings.\n",
        "\n",
        "# Corrected query using the actual column names from your BigQuery table\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "  CAST(ROW_NUMBER() OVER() AS STRING) AS faq_id,\n",
        "  string_field_0 AS question,  -- Map string_field_0 to 'question' alias\n",
        "  string_field_1 AS answer     -- Map string_field_1 to 'answer' alias\n",
        "FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`\n",
        "\"\"\"\n",
        "faq_rows = list(bq_client.query(query))\n",
        "\n",
        "print(f\"Generating embeddings for {len(faq_rows)} rows...\")\n",
        "\n",
        "rows_to_insert = []\n",
        "\n",
        "for row in faq_rows:\n",
        "    # Access the aliased column names\n",
        "    faq_id = row[\"faq_id\"]\n",
        "    question = row[\"question\"] # Now 'question' will hold the value from string_field_0\n",
        "    answer = row[\"answer\"]     # Now 'answer' will hold the value from string_field_1\n",
        "    combined_text = f\"Q: {question}\\nA: {answer}\"\n",
        "\n",
        "     # üß† Call the embeddings model\n",
        "    embedding_response = embedding_model.get_embeddings([combined_text])\n",
        "    embedding = embedding_response[0].values\n",
        "\n",
        "    rows_to_insert.append(\n",
        "        {\n",
        "            \"faq_id\": faq_id,\n",
        "            \"question\": question,\n",
        "            \"answer\": answer,\n",
        "            \"combined_text\": combined_text,\n",
        "            \"embedding\": embedding,\n",
        "        }\n",
        "    )\n",
        "# Insert into BigQuery\n",
        "errors = bq_client.insert_rows_json(\n",
        "    f\"{PROJECT_ID}.{DATASET_ID}.{EMBEDDING_TABLE_ID}\",\n",
        "    rows_to_insert,\n",
        ")\n",
        "\n",
        "if errors:\n",
        "    print(\"‚ùå Errors while inserting embeddings:\", errors)\n",
        "else:\n",
        "    print(\"‚úÖ Successfully stored embeddings in BigQuery\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-8pwxEGnwJI",
        "outputId": "754d2835-8c33-41c3-9c6f-af9442057b29"
      },
      "id": "0-8pwxEGnwJI",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating embeddings for 50 rows...\n",
            "‚úÖ Successfully stored embeddings in BigQuery\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement vector search in BigQuery"
      ],
      "metadata": {
        "id": "Pi7rXrAfn8kP"
      },
      "id": "Pi7rXrAfn8kP"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_query_embedding(query_text: str) -> list[float]:\n",
        "    \"\"\"Generate an embedding vector for the user query using the same model.\"\"\"\n",
        "    response = embedding_model.get_embeddings([query_text])\n",
        "    return response[0].values\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def search_faqs(query_text: str, top_k: int = 5):\n",
        "    \"\"\"Return top_k most similar FAQs from BigQuery using vector similarity.\"\"\"\n",
        "    query_embedding = get_query_embedding(query_text)\n",
        "\n",
        "    # BigQuery can't accept lists directly, so we pass as an array via UNNEST\n",
        "    embedding_str = \", \".join(str(x) for x in query_embedding)\n",
        "\n",
        "    # Note: we‚Äôll compute dot product between query embedding and stored embedding\n",
        "    # using ARRAY functions in BigQuery.\n",
        "    search_query = f\"\"\"\n",
        "    WITH query_embedding AS (\n",
        "      SELECT ARRAY[{embedding_str}] AS embedding\n",
        "    )\n",
        "    SELECT\n",
        "      f.faq_id,\n",
        "      f.question,\n",
        "      f.answer,\n",
        "      (\n",
        "        SELECT SUM(qe * fe)\n",
        "        FROM UNNEST(f.embedding) AS fe\n",
        "        WITH OFFSET pos\n",
        "        JOIN UNNEST(q.embedding) AS qe\n",
        "        WITH OFFSET pos2\n",
        "        ON pos = pos2\n",
        "      ) AS dot_product\n",
        "    FROM `{PROJECT_ID}.{DATASET_ID}.{EMBEDDING_TABLE_ID}` AS f\n",
        "    CROSS JOIN query_embedding AS q\n",
        "    ORDER BY dot_product DESC\n",
        "    LIMIT {top_k}\n",
        "    \"\"\"\n",
        "\n",
        "    results = list(bq_client.query(search_query))\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "U_tu8F-Wn6oR"
      },
      "id": "U_tu8F-Wn6oR",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call Gemini to answer using retrieved context"
      ],
      "metadata": {
        "id": "DhSKLhlToGNd"
      },
      "id": "DhSKLhlToGNd"
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.generative_models import GenerativeModel\n",
        "\n",
        "RESPONSE_MODEL_NAME = \"gemini-2.5-flash\"\n",
        "chat_model = GenerativeModel(RESPONSE_MODEL_NAME)\n",
        "\n",
        "def build_context_from_results(results) -> str:\n",
        "    \"\"\"Combine retrieved FAQs into a context string for Gemini.\"\"\"\n",
        "    context_chunks = []\n",
        "    for r in results:\n",
        "        context_chunks.append(\n",
        "            f\"FAQ ID: {r['faq_id']}\\nQuestion: {r['question']}\\nAnswer: {r['answer']}\"\n",
        "        )\n",
        "    return \"\\n\\n---\\n\\n\".join(context_chunks)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG47SZuvoIRN",
        "outputId": "d1735f1e-c3a8-4d9d-e875-49a15bf607b2"
      },
      "id": "gG47SZuvoIRN",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_with_rag(user_question: str) -> str:\n",
        "    \"\"\"Use BigQuery vector search + Gemini to answer the user question.\"\"\"\n",
        "    # 1Ô∏è‚É£ Retrieve top similar FAQs\n",
        "    search_results = search_faqs(user_question, top_k=5)\n",
        "    if not search_results:\n",
        "        return \"I couldn't find any relevant information in the Aurora Bay FAQ data.\"\n",
        "\n",
        "    # 2Ô∏è‚É£ Build context string\n",
        "    context = build_context_from_results(search_results)\n",
        "\n",
        "    # 3Ô∏è‚É£ Prompt Gemini with RAG-style prompt\n",
        "    system_instructions = \"\"\"\n",
        "You are a helpful assistant answering questions about the town of Aurora Bay, Alaska.\n",
        "Use ONLY the information in the 'Context' section below.\n",
        "If the answer is not clearly in the context, say you don't know.\n",
        "Respond in a concise and friendly way.\n",
        "\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"{system_instructions}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "User question:\n",
        "{user_question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "    response = chat_model.generate_content(prompt)\n",
        "    return response.text\n"
      ],
      "metadata": {
        "id": "SxtCajLUoj3Y"
      },
      "id": "SxtCajLUoj3Y",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot():\n",
        "    print(\"Aurora Bay FAQ chatbot. Ask a question, or type 'exit' to quit.\\n\")\n",
        "    while True:\n",
        "        user_q = input(\"You: \")\n",
        "        if user_q.lower().strip() in {\"exit\", \"quit\"}:\n",
        "            print(\"Bye!\")\n",
        "            break\n",
        "        answer = answer_with_rag(user_q)\n",
        "        print(f\"Bot: {answer}\\n\")\n",
        "\n",
        "# Uncomment to run interactively in the notebook:\n",
        "chatbot()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTr_B2wDomg7",
        "outputId": "b63f1897-9457-466f-a6a1-bab89d6dc726"
      },
      "id": "HTr_B2wDomg7",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aurora Bay FAQ chatbot. Ask a question, or type 'exit' to quit.\n",
            "\n",
            "You: what is 1 + 1?\n",
            "Bot: I don't know.\n",
            "\n",
            "You: When was Aurora Bay founded?\n",
            "Bot: Aurora Bay was founded in 1901.\n",
            "\n",
            "You: bye\n",
            "Bot: I don't know.\n",
            "\n",
            "You: exit\n",
            "Bye!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "student-04-ba82971ad805 (Dec 4, 2025, 1:20:46‚ÄØPM)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}